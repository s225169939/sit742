{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FK3_qvIrh3Lz"
      },
      "source": [
        "![Cloud-First](https://github.com/tulip-lab/sit742/blob/develop/Jupyter/image/CloudFirst.png?raw=1)\n",
        "\n",
        "\n",
        "# SIT742: Modern Data Science\n",
        "**(Module: Big Data Manipulation)**\n",
        "\n",
        "---\n",
        "- Materials in this module include resources collected from various open-source online repositories.\n",
        "- You are free to use, change and distribute this package.\n",
        "- If you found any issue/bug for this document, please submit an issue at [tulip-lab/sit742](https://github.com/tulip-lab/sit742/issues)\n",
        "\n",
        "\n",
        "Prepared by **SIT742 Teaching Team**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## Session 4G: SparkSQL and Data Understanding\n",
        "---\n",
        "\n",
        "### Table of Content\n",
        "\n",
        "Part A: SparkSQL\n",
        "\n",
        "1. Loading in a DataFrame\n",
        "\n",
        "2. Creating SQLContext\n",
        "\n",
        "3. Creating DataFrame\n",
        "\n",
        "4. Grouping Aggregation\n",
        "\n",
        "5. Running SQL Queries\n",
        "\n",
        "Part B: SparkSQL Application\n",
        "\n",
        "6.  Getting the data and creating the RDD\n",
        "\n",
        "7. Getting a Data Frame\n",
        "\n",
        "8. Queries as DataFrame operations\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## Introduction ##\n",
        "\n",
        "This notebook will introduce Spark capabilities to deal with data in a structured way. Basically, everything turns around the concept of *Data Frame* and using *SQL language* to query them. We will see how the data frame abstraction, very popular in other data analytics ecosystems (e.g. R and Python/Pandas), it is very powerful when performing exploratory data analysis.\n",
        "\n",
        "In fact, it is very easy to express data queries when used together with the SQL language. Moreover, Spark distributes this column-based data structure transparently, in order to make the querying process as efficient as possible.      \n",
        "\n",
        "This lab session will assume that you have uploaded two data files into cloud, and note down the address:\n",
        "- mtcars.csv\n",
        "- kddcup_data_10_percent-d8e1d.gz\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LtLolzbVY3j9"
      },
      "source": [
        "# Part A: SparkSQL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwTeJHXPh3L2"
      },
      "source": [
        "### 1.Loading in a DataFrame\n",
        "To create a Spark DataFrame we load an external DataFrame, called `mtcars`. This DataFrame includes 32 observations on 11 variables.\n",
        "\n",
        "[, 1]\tmpg\tMiles/(US) --> gallon  \n",
        "[, 2]\tcyl\t--> Number of cylinders  \n",
        "[, 3]\tdisp\t--> Displacement (cu.in.)  \n",
        "[, 4]\thp -->\tGross horsepower  \n",
        "[, 5]\tdrat -->\tRear axle ratio  \n",
        "[, 6]\twt -->\tWeight (lb/1000)  \n",
        "[, 7]\tqsec -->\t1/4 mile time  \n",
        "[, 8]\tvs -->\tV/S  \n",
        "[, 9]\tam -->\tTransmission (0 = automatic, 1 = manual)  \n",
        "[,10]\tgear -->\tNumber of forward gears  \n",
        "[,11]\tcarb -->\tNumber of carburetors  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2IVBZtPYh3L3",
        "outputId": "72b54315-290b-439e-f6f0-e2cfce3fe1c1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wget\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9655 sha256=e6483974e783f51fbe37f6bf6e097f0d5d9fd99b93020bdcdd5617657e11641f\n",
            "  Stored in directory: /root/.cache/pip/wheels/01/46/3b/e29ffbe4ebe614ff224bad40fc6a5773a67a163251585a13a9\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n"
          ]
        }
      ],
      "source": [
        "!pip install wget"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "K4pcGZbRh3L7"
      },
      "outputs": [],
      "source": [
        "import wget\n",
        "\n",
        "link_to_data = 'https://raw.githubusercontent.com/tulip-lab/sit742/refs/heads/develop/Jupyter/data/mtcars.csv'\n",
        "DataSet = wget.download(link_to_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ZgQBUBGmh3L-"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "mtcars = pd.read_csv('mtcars.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "8bn_g86Bh3MA",
        "scrolled": true,
        "outputId": "bd3297d7-1b97-4e32-ab41-db46c2cb4167",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                 car   mpg  cyl   disp   hp  drat     wt   qsec  vs  am  gear  \\\n",
              "0          Mazda RX4  21.0    6  160.0  110  3.90  2.620  16.46   0   1     4   \n",
              "1      Mazda RX4 Wag  21.0    6  160.0  110  3.90  2.875  17.02   0   1     4   \n",
              "2         Datsun 710  22.8    4  108.0   93  3.85  2.320  18.61   1   1     4   \n",
              "3     Hornet 4 Drive  21.4    6  258.0  110  3.08  3.215  19.44   1   0     3   \n",
              "4  Hornet Sportabout  18.7    8  360.0  175  3.15  3.440  17.02   0   0     3   \n",
              "\n",
              "   carb  \n",
              "0     4  \n",
              "1     4  \n",
              "2     1  \n",
              "3     1  \n",
              "4     2  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-36aac3a5-ec3e-4ccc-b400-5794925b4654\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>car</th>\n",
              "      <th>mpg</th>\n",
              "      <th>cyl</th>\n",
              "      <th>disp</th>\n",
              "      <th>hp</th>\n",
              "      <th>drat</th>\n",
              "      <th>wt</th>\n",
              "      <th>qsec</th>\n",
              "      <th>vs</th>\n",
              "      <th>am</th>\n",
              "      <th>gear</th>\n",
              "      <th>carb</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Mazda RX4</td>\n",
              "      <td>21.0</td>\n",
              "      <td>6</td>\n",
              "      <td>160.0</td>\n",
              "      <td>110</td>\n",
              "      <td>3.90</td>\n",
              "      <td>2.620</td>\n",
              "      <td>16.46</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Mazda RX4 Wag</td>\n",
              "      <td>21.0</td>\n",
              "      <td>6</td>\n",
              "      <td>160.0</td>\n",
              "      <td>110</td>\n",
              "      <td>3.90</td>\n",
              "      <td>2.875</td>\n",
              "      <td>17.02</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Datsun 710</td>\n",
              "      <td>22.8</td>\n",
              "      <td>4</td>\n",
              "      <td>108.0</td>\n",
              "      <td>93</td>\n",
              "      <td>3.85</td>\n",
              "      <td>2.320</td>\n",
              "      <td>18.61</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Hornet 4 Drive</td>\n",
              "      <td>21.4</td>\n",
              "      <td>6</td>\n",
              "      <td>258.0</td>\n",
              "      <td>110</td>\n",
              "      <td>3.08</td>\n",
              "      <td>3.215</td>\n",
              "      <td>19.44</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Hornet Sportabout</td>\n",
              "      <td>18.7</td>\n",
              "      <td>8</td>\n",
              "      <td>360.0</td>\n",
              "      <td>175</td>\n",
              "      <td>3.15</td>\n",
              "      <td>3.440</td>\n",
              "      <td>17.02</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-36aac3a5-ec3e-4ccc-b400-5794925b4654')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-36aac3a5-ec3e-4ccc-b400-5794925b4654 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-36aac3a5-ec3e-4ccc-b400-5794925b4654');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-26d46479-a371-43d6-b4a6-eb9a264053aa\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-26d46479-a371-43d6-b4a6-eb9a264053aa')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-26d46479-a371-43d6-b4a6-eb9a264053aa button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "mtcars",
              "summary": "{\n  \"name\": \"mtcars\",\n  \"rows\": 32,\n  \"fields\": [\n    {\n      \"column\": \"car\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 32,\n        \"samples\": [\n          \"Ferrari Dino\",\n          \"Lincoln Continental\",\n          \"Pontiac Firebird\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mpg\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 6.026948052089105,\n        \"min\": 10.4,\n        \"max\": 33.9,\n        \"num_unique_values\": 25,\n        \"samples\": [\n          17.8,\n          33.9,\n          21.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"cyl\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 4,\n        \"max\": 8,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          6,\n          4,\n          8\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"disp\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 123.93869383138194,\n        \"min\": 71.1,\n        \"max\": 472.0,\n        \"num_unique_values\": 27,\n        \"samples\": [\n          275.8,\n          75.7,\n          472.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"hp\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 68,\n        \"min\": 52,\n        \"max\": 335,\n        \"num_unique_values\": 22,\n        \"samples\": [\n          110,\n          52,\n          180\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"drat\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.5346787360709716,\n        \"min\": 2.76,\n        \"max\": 4.93,\n        \"num_unique_values\": 22,\n        \"samples\": [\n          3.9,\n          4.93,\n          3.07\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"wt\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.9784574429896967,\n        \"min\": 1.513,\n        \"max\": 5.424,\n        \"num_unique_values\": 29,\n        \"samples\": [\n          2.77,\n          1.615,\n          5.25\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"qsec\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.7869432360968431,\n        \"min\": 14.5,\n        \"max\": 22.9,\n        \"num_unique_values\": 30,\n        \"samples\": [\n          15.5,\n          17.42,\n          17.05\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"vs\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"am\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"gear\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 3,\n        \"max\": 5,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          4,\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"carb\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 8,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          4,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "mtcars.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VLqidgEsh3MC"
      },
      "source": [
        "### 2.Initialize SQLContext\n",
        "To work with dataframes we need a SQLContext which is created using `SQLContext(sc)`. SQLContext uses SparkContext which was the main entry point for Spark functionality, named `sc`.\n",
        "\n",
        "A SparkContext represents the connection to a Spark cluster, and can be used to create RDDs, accumulators and broadcast variables on that cluster. Only one SparkContext may be active per JVM. You must stop() the active SparkContext before creating a new one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "hiTfYwOpinyN",
        "outputId": "da7c02f1-fe31-47c4-cf06-662f6119fa13",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 https://cli.github.com/packages stable InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com] [Waiting for headers] [Connected to cloud\r                                                                               \rGet:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,628 B]\n",
            "Get:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:4 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:10 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,940 kB]\n",
            "Hit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:13 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,197 kB]\n",
            "Get:14 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,782 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,253 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,271 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [5,519 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,576 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,606 kB]\n",
            "Fetched 29.5 MB in 7s (4,314 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n"
          ]
        }
      ],
      "source": [
        "#update local version of the package catalog\n",
        "!apt-get update\n",
        "# install Java8\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "# download spark 3.3.3\n",
        "# !wget -q https://archive.apache.org/dist/spark/spark-3.3.3/spark-3.3.3-bin-hadoop3.tgz\n",
        "# # unzip it\n",
        "# !tar xf spark-3.3.3-bin-hadoop3.tgz\n",
        "# install findspark\n",
        "!pip install -q findspark\n",
        "\n",
        "import os\n",
        "# os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "# os.environ[\"SPARK_HOME\"] = \"/content/spark-3.3.3-bin-hadoop3\"\n",
        "\n",
        "import findspark\n",
        "findspark.init()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "JpFoyri0h3MD",
        "outputId": "1d3fe3cd-43be-4387-8a5d-195dc9dc2828",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/pyspark/sql/context.py:113: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from pyspark import SparkContext\n",
        "from pyspark.sql import SQLContext\n",
        "\n",
        "#The getOrCreate function for the SparkContext may be used to get or instantiate a SparkContext and register it as a singleton object.\n",
        "sc = SparkContext.getOrCreate()\n",
        "\n",
        "#SQLContext is the entry point for working with structured data (rows and columns) in Spark, in Spark 1.x.\n",
        "#As of Spark 2.0, this is replaced by SparkSession.\n",
        "#However, we are keeping the class here for backward compatibility.\n",
        "#A SQLContext can be used create DataFrame, register DataFrame as tables, execute SQL over tables, cache tables, and read parquet files\n",
        "sqlContext = SQLContext(sc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "doJ6p8Shh3MF"
      },
      "source": [
        "### 3.Creating Spark DataFrames\n",
        "With SQLContext and a loaded local DataFrame, we create a Spark DataFrame:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "9ECTDKYTh3MG",
        "outputId": "90b7370b-03d2-4c8e-c426-d4a7aa6cceb9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- am: long (nullable = true)\n",
            " |-- car: string (nullable = true)\n",
            " |-- carb: long (nullable = true)\n",
            " |-- cyl: long (nullable = true)\n",
            " |-- disp: double (nullable = true)\n",
            " |-- drat: double (nullable = true)\n",
            " |-- gear: long (nullable = true)\n",
            " |-- hp: long (nullable = true)\n",
            " |-- mpg: double (nullable = true)\n",
            " |-- qsec: double (nullable = true)\n",
            " |-- vs: long (nullable = true)\n",
            " |-- wt: double (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Creates a DataFrame from an RDD, a list or a pandas.DataFrame\n",
        "sdf = sqlContext.createDataFrame(mtcars.to_dict('records'))\n",
        "#Prints out the schema in the tree format.\n",
        "sdf.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jE7OEjWLh3MH"
      },
      "source": [
        "You can also directly load this csv file into a Spark DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "GN2d6lXBh3MJ",
        "outputId": "83c7419f-0bef-485a-90c7-79dfe4c5eb19",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame[_c0: string, _c1: string, _c2: string, _c3: string, _c4: string, _c5: string, _c6: string, _c7: string, _c8: string, _c9: string, _c10: string, _c11: string]\n",
            "DataFrame[_c0: string, _c1: string, _c2: string, _c3: string, _c4: string, _c5: string, _c6: string, _c7: string, _c8: string, _c9: string, _c10: string, _c11: string]\n"
          ]
        }
      ],
      "source": [
        "#Sample 1\n",
        "#Returns a DataFrameReader that can be used to read data in as a DataFrame.\n",
        "sdf2 = sqlContext.read.csv(\"mtcars.csv\")\n",
        "print(sdf2)\n",
        "\n",
        "\n",
        "#Sample 2 to define a specific format\n",
        "#Specifies the input data source format.\n",
        "sdf3=sqlContext.read.format('csv').load('mtcars.csv')\n",
        "print(sdf3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FB64EqmZh3MK"
      },
      "source": [
        "#### (3a) Displays the content of the DataFrame\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "-vnHoYeWh3ML",
        "scrolled": true,
        "outputId": "30ee6a1f-0827-4f46-bbe7-a239be6b0101",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----------------+----+---+-----+----+----+---+----+-----+---+-----+\n",
            "| am|              car|carb|cyl| disp|drat|gear| hp| mpg| qsec| vs|   wt|\n",
            "+---+-----------------+----+---+-----+----+----+---+----+-----+---+-----+\n",
            "|  1|        Mazda RX4|   4|  6|160.0| 3.9|   4|110|21.0|16.46|  0| 2.62|\n",
            "|  1|    Mazda RX4 Wag|   4|  6|160.0| 3.9|   4|110|21.0|17.02|  0|2.875|\n",
            "|  1|       Datsun 710|   1|  4|108.0|3.85|   4| 93|22.8|18.61|  1| 2.32|\n",
            "|  0|   Hornet 4 Drive|   1|  6|258.0|3.08|   3|110|21.4|19.44|  1|3.215|\n",
            "|  0|Hornet Sportabout|   2|  8|360.0|3.15|   3|175|18.7|17.02|  0| 3.44|\n",
            "+---+-----------------+----+---+-----+----+----+---+----+-----+---+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "sdf.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5EyxC_8h3MM"
      },
      "source": [
        "#### (3b) Selecting columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "22cyMzU2h3MN",
        "scrolled": true,
        "outputId": "0c0ad81d-b2b1-4d6f-ee90-7ce1669a93b1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+\n",
            "| mpg|\n",
            "+----+\n",
            "|21.0|\n",
            "|21.0|\n",
            "|22.8|\n",
            "|21.4|\n",
            "|18.7|\n",
            "+----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "sdf.select('mpg').show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDAw-uUth3MO"
      },
      "source": [
        "#### (3c)  Filtering Data\n",
        "Filter the DataFrame to only retain rows with `mpg` less than 18"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "uwTmPCmnh3MP",
        "outputId": "de9010eb-cac1-40ef-ff26-fcb1a39db927",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----------+----+---+-----+----+----+---+----+-----+---+----+\n",
            "| am|        car|carb|cyl| disp|drat|gear| hp| mpg| qsec| vs|  wt|\n",
            "+---+-----------+----+---+-----+----+----+---+----+-----+---+----+\n",
            "|  0| Duster 360|   4|  8|360.0|3.21|   3|245|14.3|15.84|  0|3.57|\n",
            "|  0|  Merc 280C|   4|  6|167.6|3.92|   4|123|17.8| 18.9|  1|3.44|\n",
            "|  0| Merc 450SE|   3|  8|275.8|3.07|   3|180|16.4| 17.4|  0|4.07|\n",
            "|  0| Merc 450SL|   3|  8|275.8|3.07|   3|180|17.3| 17.6|  0|3.73|\n",
            "|  0|Merc 450SLC|   3|  8|275.8|3.07|   3|180|15.2| 18.0|  0|3.78|\n",
            "+---+-----------+----+---+-----+----+----+---+----+-----+---+----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "sdf.filter(sdf['mpg'] < 18).show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5oJ2FWjh3MR"
      },
      "source": [
        "#### (3d)  Operating on Columns\n",
        "Spark also provides a number of functions that can directly applied to columns for data processing and aggregation. The example below shows the use of basic arithmetic functions to convert lb to metric ton."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "iEsLIAAvh3MS",
        "outputId": "57105fd1-592e-491d-8e7b-21e760deaa6f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----------------+----+---+-----+----+----+---+----+-----+---+-----+-------+\n",
            "| am|              car|carb|cyl| disp|drat|gear| hp| mpg| qsec| vs|   wt|  wtTon|\n",
            "+---+-----------------+----+---+-----+----+----+---+----+-----+---+-----+-------+\n",
            "|  1|        Mazda RX4|   4|  6|160.0| 3.9|   4|110|21.0|16.46|  0| 2.62|  1.179|\n",
            "|  1|    Mazda RX4 Wag|   4|  6|160.0| 3.9|   4|110|21.0|17.02|  0|2.875|1.29375|\n",
            "|  1|       Datsun 710|   1|  4|108.0|3.85|   4| 93|22.8|18.61|  1| 2.32|  1.044|\n",
            "|  0|   Hornet 4 Drive|   1|  6|258.0|3.08|   3|110|21.4|19.44|  1|3.215|1.44675|\n",
            "|  0|Hornet Sportabout|   2|  8|360.0|3.15|   3|175|18.7|17.02|  0| 3.44|  1.548|\n",
            "|  0|          Valiant|   1|  6|225.0|2.76|   3|105|18.1|20.22|  1| 3.46|  1.557|\n",
            "+---+-----------------+----+---+-----+----+----+---+----+-----+---+-----+-------+\n",
            "only showing top 6 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# A new clomun name 'wtTon' is created and its value equals 'wt' * 0.45\n",
        "sdf.withColumn('wtTon', sdf['wt'] * 0.45).show(6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "U7jQbGjah3MU",
        "outputId": "2e221bc0-c543-4c3f-cb4c-df96d7f0b029",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----------------+----+---+-----+----+----+---+----+-----+---+-----+\n",
            "| am|              car|carb|cyl| disp|drat|gear| hp| mpg| qsec| vs|   wt|\n",
            "+---+-----------------+----+---+-----+----+----+---+----+-----+---+-----+\n",
            "|  1|        Mazda RX4|   4|  6|160.0| 3.9|   4|110|21.0|16.46|  0| 2.62|\n",
            "|  1|    Mazda RX4 Wag|   4|  6|160.0| 3.9|   4|110|21.0|17.02|  0|2.875|\n",
            "|  1|       Datsun 710|   1|  4|108.0|3.85|   4| 93|22.8|18.61|  1| 2.32|\n",
            "|  0|   Hornet 4 Drive|   1|  6|258.0|3.08|   3|110|21.4|19.44|  1|3.215|\n",
            "|  0|Hornet Sportabout|   2|  8|360.0|3.15|   3|175|18.7|17.02|  0| 3.44|\n",
            "|  0|          Valiant|   1|  6|225.0|2.76|   3|105|18.1|20.22|  1| 3.46|\n",
            "+---+-----------------+----+---+-----+----+----+---+----+-----+---+-----+\n",
            "only showing top 6 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "sdf.show(6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTYersd9h3MW"
      },
      "source": [
        "###4.Grouping, Aggregation\n",
        "Spark DataFrames support a number of commonly used functions to aggregate data after grouping. For example we can compute the average weight of cars by their cylinders as shown below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "jV5KWdv2h3MX",
        "outputId": "21df9237-4aa2-4497-fcae-68b68454707b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----------------+\n",
            "|cyl|          avg(wt)|\n",
            "+---+-----------------+\n",
            "|  6|3.117142857142857|\n",
            "|  8|3.999214285714286|\n",
            "|  4|2.285727272727273|\n",
            "+---+-----------------+\n",
            "\n",
            "+---+-----------------+\n",
            "|cyl|          avg(wt)|\n",
            "+---+-----------------+\n",
            "|  6|3.117142857142857|\n",
            "|  8|3.999214285714286|\n",
            "|  4|2.285727272727273|\n",
            "+---+-----------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "sdf.groupby(['cyl'])\\\n",
        ".agg({\"wt\": \"AVG\"})\\\n",
        ".show(5)\n",
        "\n",
        "#It also equals the below line:\n",
        "sdf.groupby(['cyl']).agg({\"wt\": \"AVG\"}).show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "xRCXT1ykh3Md",
        "outputId": "7461d489-57e6-4687-8eed-486b3d6aa36c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---------+\n",
            "|cyl|count(wt)|\n",
            "+---+---------+\n",
            "|  8|       14|\n",
            "|  4|       11|\n",
            "|  6|        7|\n",
            "+---+---------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# We can also sort the output from the aggregation to get the most common cars\n",
        "car_counts = sdf.groupby(['cyl'])\\\n",
        ".agg({\"wt\": \"count\"})\\\n",
        ".sort(\"count(wt)\", ascending=False)\\\n",
        ".show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8T2LxMih3Mg"
      },
      "source": [
        "### 5.Running SQL Queries from Spark DataFrames\n",
        "A Spark DataFrame can also be registered as a temporary table in Spark SQL and registering a DataFrame as a table allows you to run SQL queries over its data. The `sql` function enables applications to run SQL queries programmatically and returns the result as a DataFrame.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "nS2N_w0ph3Mh",
        "outputId": "18fe2773-5120-46c8-9085-1b72cd11693b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/pyspark/sql/dataframe.py:329: FutureWarning: Deprecated in 2.0, use createOrReplaceTempView instead.\n",
            "  warnings.warn(\"Deprecated in 2.0, use createOrReplaceTempView instead.\", FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+\n",
            "|gear|\n",
            "+----+\n",
            "|   4|\n",
            "|   4|\n",
            "|   4|\n",
            "|   3|\n",
            "|   3|\n",
            "|   3|\n",
            "+----+\n",
            "only showing top 6 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Register this DataFrame as a table.\n",
        "sdf.registerTempTable(\"cars\")\n",
        "\n",
        "# SQL statements can be run by using the sql method\n",
        "highgearcars = sqlContext.sql(\"SELECT gear FROM cars WHERE cyl >= 4 AND cyl <= 9\")\n",
        "highgearcars.show(6)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUsyGXlYh3Mj"
      },
      "source": [
        "## Part B: SparkSQL Application"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3S65v5gsh3Mk"
      },
      "source": [
        "### 6.Getting the data and creating the RDD"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMfA3XGDh3Ml"
      },
      "source": [
        "As we did in previous notebooks, we will use the reduced dataset (10 percent) provided for the [KDD Cup 1999](http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html), containing nearly half million network interactions. The file is provided as a Gzip file that we will download from the GitHub.\n",
        "\n",
        "You can also use the urllib to download the orginal Gzip file as below codes:\n",
        "```\n",
        "import urllib\n",
        "f = urllib.urlretrieve (\"http://kdd.ics.uci.edu/databases/kddcup99/kddcup.data_10_percent.gz\", \"kddcup.data_10_percent.gz\")\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "OT5mRphWh3Mm"
      },
      "outputs": [],
      "source": [
        "import wget\n",
        "link_to_data = 'https://github.com/tulip-lab/sit742/raw/develop/Jupyter/data/kddcup.gz'\n",
        "DataSet = wget.download(link_to_data, out='kdd.gz')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "7TtPuauZh3Mn",
        "outputId": "d0f39494-55e9-445e-bf77-424815c70f46",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 2104\n",
            "-rw-r--r-- 1 root root 2144903 Aug 27 11:53 kdd.gz\n",
            "-rw-r--r-- 1 root root    1697 Aug 27 11:51 mtcars.csv\n",
            "drwxr-xr-x 1 root root    4096 Aug 22 13:37 sample_data\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "!ls -l\n",
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "rGBn15krh3Mp"
      },
      "outputs": [],
      "source": [
        "data_file = \"kdd.gz\"\n",
        "\n",
        "#textFile is used to read a text file from HDFS,\n",
        "#a local file system (available on all nodes),\n",
        "#or any Hadoop-supported file system URI, and return it as an RDD of Strings.\n",
        "#In here, we assume that the file \"kdd.gz\" haved been in the HDFS\n",
        "raw_data = sc.textFile(data_file).cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBHMNiQfh3Mq"
      },
      "source": [
        "### 7.Getting a Data Frame"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0L5eo9qh3Mr"
      },
      "source": [
        "A Spark `DataFrame` is a distributed collection of data organized into named columns. It is conceptually equivalent to a table in a relational database or a data frame in R or Pandas. They can be constructed from a wide array of sources such as a existing RDD in our case."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7p2cOPXh3Mr"
      },
      "source": [
        "The entry point into all SQL functionality in Spark is the `SQLContext` class. To create a basic instance, all we need is a `SparkContext` reference. Since we are running Spark in shell mode (using pySpark) we can use the global context object `sc` for this purpose.    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "e-n3zqZdh3Ms",
        "outputId": "4055089c-3d56-4039-fc7c-2f19cf7f1d5c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/pyspark/sql/context.py:113: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SQLContext\n",
        "sqlContext = SQLContext(sc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38t_qtE5h3Mu"
      },
      "source": [
        "#### (7a) Inferring the schema"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HOTn61x2h3Mv"
      },
      "source": [
        "With a `SQLContext`, we are ready to create a `DataFrame` from our existing RDD. But first we need to tell Spark SQL the schema in our data.   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VLh3U1Xh3Mv"
      },
      "source": [
        "Spark SQL can convert an RDD of `Row` objects to a `DataFrame`. Rows are constructed by passing a list of key/value pairs as *kwargs* to the `Row` class. The keys define the column names, and the types are inferred by looking at the first row. Therefore, it is important that there is no missing data in the first row of the RDD in order to properly infer the schema."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "us4USMoRh3Mw"
      },
      "source": [
        "In our case, we first need to split the comma separated data, and then use the information in KDD's 1999 task description to obtain the [column names](http://kdd.ics.uci.edu/databases/kddcup99/kddcup.names).  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "HQmuTXp_h3Mw"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import Row\n",
        "\n",
        "\n",
        "#Map is used to return an iterator that applies function to every item of iterable, yielding the results.\n",
        "#If additional iterable arguments are passed, function must take that many arguments and is applied to the items from all iterables in parallel.\n",
        "#With multiple iterables, the iterator stops when the shortest iterable is exhausted.\n",
        "csv_data = raw_data.map(lambda l: l.split(\",\"))\n",
        "row_data = csv_data.map(lambda p: Row(\n",
        "    duration=int(p[0]),\n",
        "    protocol_type=p[1],\n",
        "    service=p[2],\n",
        "    flag=p[3],\n",
        "    src_bytes=int(p[4]),\n",
        "    dst_bytes=int(p[5])\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "fh3QDIN7h3My",
        "outputId": "9b7cb587-f014-493a-c247-4a4c812c0919",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on SQLContext in module pyspark.sql.context object:\n",
            "\n",
            "class SQLContext(builtins.object)\n",
            " |  SQLContext(sparkContext: pyspark.context.SparkContext, sparkSession: Optional[pyspark.sql.session.SparkSession] = None, jsqlContext: Optional[py4j.java_gateway.JavaObject] = None)\n",
            " |\n",
            " |  The entry point for working with structured data (rows and columns) in Spark, in Spark 1.x.\n",
            " |\n",
            " |  As of Spark 2.0, this is replaced by :class:`SparkSession`. However, we are keeping the class\n",
            " |  here for backward compatibility.\n",
            " |\n",
            " |  A SQLContext can be used to create :class:`DataFrame`, register :class:`DataFrame` as\n",
            " |  tables, execute SQL over tables, cache tables, and read parquet files.\n",
            " |\n",
            " |  .. deprecated:: 3.0.0\n",
            " |      Use :func:`SparkSession.builder.getOrCreate()` instead.\n",
            " |\n",
            " |  Parameters\n",
            " |  ----------\n",
            " |  sparkContext : :class:`SparkContext`\n",
            " |      The :class:`SparkContext` backing this SQLContext.\n",
            " |  sparkSession : :class:`SparkSession`\n",
            " |      The :class:`SparkSession` around which this SQLContext wraps.\n",
            " |  jsqlContext : optional\n",
            " |      An optional JVM Scala SQLContext. If set, we do not instantiate a new\n",
            " |      SQLContext in the JVM, instead we make all calls to this object.\n",
            " |      This is only for internal.\n",
            " |\n",
            " |  Examples\n",
            " |  --------\n",
            " |  >>> from datetime import datetime\n",
            " |  >>> from pyspark.sql import Row\n",
            " |  >>> sqlContext = SQLContext(sc)\n",
            " |  >>> allTypes = sc.parallelize([Row(i=1, s=\"string\", d=1.0, l=1,\n",
            " |  ...     b=True, list=[1, 2, 3], dict={\"s\": 0}, row=Row(a=1),\n",
            " |  ...     time=datetime(2014, 8, 1, 14, 1, 5))])\n",
            " |  >>> df = allTypes.toDF()\n",
            " |  >>> df.createOrReplaceTempView(\"allTypes\")\n",
            " |  >>> sqlContext.sql('select i+1, d+1, not b, list[1], dict[\"s\"], time, row.a '\n",
            " |  ...            'from allTypes where b and i > 0').collect()\n",
            " |  [Row((i + 1)=2, (d + 1)=2.0, (NOT b)=False, list[1]=2,         dict[s]=0, time=datetime.datetime(2014, 8, 1, 14, 1, 5), a=1)]\n",
            " |  >>> df.rdd.map(lambda x: (x.i, x.s, x.d, x.l, x.b, x.time, x.row.a, x.list)).collect()\n",
            " |  [(1, 'string', 1.0, 1, True, datetime.datetime(2014, 8, 1, 14, 1, 5), 1, [1, 2, 3])]\n",
            " |\n",
            " |  Methods defined here:\n",
            " |\n",
            " |  __init__(self, sparkContext: pyspark.context.SparkContext, sparkSession: Optional[pyspark.sql.session.SparkSession] = None, jsqlContext: Optional[py4j.java_gateway.JavaObject] = None)\n",
            " |      Initialize self.  See help(type(self)) for accurate signature.\n",
            " |\n",
            " |  cacheTable(self, tableName: str) -> None\n",
            " |      Caches the specified table in-memory.\n",
            " |\n",
            " |      .. versionadded:: 1.0\n",
            " |\n",
            " |  clearCache(self) -> None\n",
            " |      Removes all cached tables from the in-memory cache.\n",
            " |\n",
            " |      .. versionadded:: 1.3\n",
            " |\n",
            " |  createDataFrame(self, data: Union[pyspark.rdd.RDD[Any], Iterable[Any], ForwardRef('PandasDataFrameLike')], schema: Union[pyspark.sql.types.AtomicType, pyspark.sql.types.StructType, str, NoneType] = None, samplingRatio: Optional[float] = None, verifySchema: bool = True) -> pyspark.sql.dataframe.DataFrame\n",
            " |      Creates a :class:`DataFrame` from an :class:`RDD`, a list or a :class:`pandas.DataFrame`.\n",
            " |\n",
            " |      When ``schema`` is a list of column names, the type of each column\n",
            " |      will be inferred from ``data``.\n",
            " |\n",
            " |      When ``schema`` is ``None``, it will try to infer the schema (column names and types)\n",
            " |      from ``data``, which should be an RDD of :class:`Row`,\n",
            " |      or :class:`namedtuple`, or :class:`dict`.\n",
            " |\n",
            " |      When ``schema`` is :class:`pyspark.sql.types.DataType` or a datatype string it must match\n",
            " |      the real data, or an exception will be thrown at runtime. If the given schema is not\n",
            " |      :class:`pyspark.sql.types.StructType`, it will be wrapped into a\n",
            " |      :class:`pyspark.sql.types.StructType` as its only field, and the field name will be \"value\",\n",
            " |      each record will also be wrapped into a tuple, which can be converted to row later.\n",
            " |\n",
            " |      If schema inference is needed, ``samplingRatio`` is used to determine the ratio of\n",
            " |      rows used for schema inference. The first row will be used if ``samplingRatio`` is ``None``.\n",
            " |\n",
            " |      .. versionadded:: 1.3.0\n",
            " |\n",
            " |      .. versionchanged:: 2.0.0\n",
            " |         The ``schema`` parameter can be a :class:`pyspark.sql.types.DataType` or a\n",
            " |         datatype string after 2.0.\n",
            " |         If it's not a :class:`pyspark.sql.types.StructType`, it will be wrapped into a\n",
            " |         :class:`pyspark.sql.types.StructType` and each record will also be wrapped into a tuple.\n",
            " |\n",
            " |      .. versionchanged:: 2.1.0\n",
            " |         Added verifySchema.\n",
            " |\n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      data : :class:`RDD` or iterable\n",
            " |          an RDD of any kind of SQL data representation (:class:`Row`,\n",
            " |          :class:`tuple`, ``int``, ``boolean``, etc.), or :class:`list`, or\n",
            " |          :class:`pandas.DataFrame`.\n",
            " |      schema : :class:`pyspark.sql.types.DataType`, str or list, optional\n",
            " |          a :class:`pyspark.sql.types.DataType` or a datatype string or a list of\n",
            " |          column names, default is None.  The data type string format equals to\n",
            " |          :class:`pyspark.sql.types.DataType.simpleString`, except that top level struct type can\n",
            " |          omit the ``struct<>``.\n",
            " |      samplingRatio : float, optional\n",
            " |          the sample ratio of rows used for inferring\n",
            " |      verifySchema : bool, optional\n",
            " |          verify data types of every row against schema. Enabled by default.\n",
            " |\n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`DataFrame`\n",
            " |\n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> l = [('Alice', 1)]\n",
            " |      >>> sqlContext.createDataFrame(l).collect()\n",
            " |      [Row(_1='Alice', _2=1)]\n",
            " |      >>> sqlContext.createDataFrame(l, ['name', 'age']).collect()\n",
            " |      [Row(name='Alice', age=1)]\n",
            " |\n",
            " |      >>> d = [{'name': 'Alice', 'age': 1}]\n",
            " |      >>> sqlContext.createDataFrame(d).collect()\n",
            " |      [Row(age=1, name='Alice')]\n",
            " |\n",
            " |      >>> rdd = sc.parallelize(l)\n",
            " |      >>> sqlContext.createDataFrame(rdd).collect()\n",
            " |      [Row(_1='Alice', _2=1)]\n",
            " |      >>> df = sqlContext.createDataFrame(rdd, ['name', 'age'])\n",
            " |      >>> df.collect()\n",
            " |      [Row(name='Alice', age=1)]\n",
            " |\n",
            " |      >>> from pyspark.sql import Row\n",
            " |      >>> Person = Row('name', 'age')\n",
            " |      >>> person = rdd.map(lambda r: Person(*r))\n",
            " |      >>> df2 = sqlContext.createDataFrame(person)\n",
            " |      >>> df2.collect()\n",
            " |      [Row(name='Alice', age=1)]\n",
            " |\n",
            " |      >>> from pyspark.sql.types import *\n",
            " |      >>> schema = StructType([\n",
            " |      ...    StructField(\"name\", StringType(), True),\n",
            " |      ...    StructField(\"age\", IntegerType(), True)])\n",
            " |      >>> df3 = sqlContext.createDataFrame(rdd, schema)\n",
            " |      >>> df3.collect()\n",
            " |      [Row(name='Alice', age=1)]\n",
            " |\n",
            " |      >>> sqlContext.createDataFrame(df.toPandas()).collect()  # doctest: +SKIP\n",
            " |      [Row(name='Alice', age=1)]\n",
            " |      >>> sqlContext.createDataFrame(pandas.DataFrame([[1, 2]])).collect()  # doctest: +SKIP\n",
            " |      [Row(0=1, 1=2)]\n",
            " |\n",
            " |      >>> sqlContext.createDataFrame(rdd, \"a: string, b: int\").collect()\n",
            " |      [Row(a='Alice', b=1)]\n",
            " |      >>> rdd = rdd.map(lambda row: row[1])\n",
            " |      >>> sqlContext.createDataFrame(rdd, \"int\").collect()\n",
            " |      [Row(value=1)]\n",
            " |      >>> sqlContext.createDataFrame(rdd, \"boolean\").collect() # doctest: +IGNORE_EXCEPTION_DETAIL\n",
            " |      Traceback (most recent call last):\n",
            " |          ...\n",
            " |      Py4JJavaError: ...\n",
            " |\n",
            " |  createExternalTable(self, tableName: str, path: Optional[str] = None, source: Optional[str] = None, schema: Optional[pyspark.sql.types.StructType] = None, **options: str) -> pyspark.sql.dataframe.DataFrame\n",
            " |      Creates an external table based on the dataset in a data source.\n",
            " |\n",
            " |      It returns the DataFrame associated with the external table.\n",
            " |\n",
            " |      The data source is specified by the ``source`` and a set of ``options``.\n",
            " |      If ``source`` is not specified, the default data source configured by\n",
            " |      ``spark.sql.sources.default`` will be used.\n",
            " |\n",
            " |      Optionally, a schema can be provided as the schema of the returned :class:`DataFrame` and\n",
            " |      created external table.\n",
            " |\n",
            " |      .. versionadded:: 1.3.0\n",
            " |\n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`DataFrame`\n",
            " |\n",
            " |  dropTempTable(self, tableName: str) -> None\n",
            " |      Remove the temporary table from catalog.\n",
            " |\n",
            " |      .. versionadded:: 1.6.0\n",
            " |\n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> sqlContext.registerDataFrameAsTable(df, \"table1\")\n",
            " |      >>> sqlContext.dropTempTable(\"table1\")\n",
            " |\n",
            " |  getConf(self, key: str, defaultValue: Union[str, NoneType, pyspark._globals._NoValueType] = <no value>) -> Optional[str]\n",
            " |      Returns the value of Spark SQL configuration property for the given key.\n",
            " |\n",
            " |      If the key is not set and defaultValue is set, return\n",
            " |      defaultValue. If the key is not set and defaultValue is not set, return\n",
            " |      the system default value.\n",
            " |\n",
            " |      .. versionadded:: 1.3.0\n",
            " |\n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> sqlContext.getConf(\"spark.sql.shuffle.partitions\")\n",
            " |      '200'\n",
            " |      >>> sqlContext.getConf(\"spark.sql.shuffle.partitions\", \"10\")\n",
            " |      '10'\n",
            " |      >>> sqlContext.setConf(\"spark.sql.shuffle.partitions\", \"50\")\n",
            " |      >>> sqlContext.getConf(\"spark.sql.shuffle.partitions\", \"10\")\n",
            " |      '50'\n",
            " |\n",
            " |  newSession(self) -> 'SQLContext'\n",
            " |      Returns a new SQLContext as new session, that has separate SQLConf,\n",
            " |      registered temporary views and UDFs, but shared SparkContext and\n",
            " |      table cache.\n",
            " |\n",
            " |      .. versionadded:: 1.6.0\n",
            " |\n",
            " |  range(self, start: int, end: Optional[int] = None, step: int = 1, numPartitions: Optional[int] = None) -> pyspark.sql.dataframe.DataFrame\n",
            " |      Create a :class:`DataFrame` with single :class:`pyspark.sql.types.LongType` column named\n",
            " |      ``id``, containing elements in a range from ``start`` to ``end`` (exclusive) with\n",
            " |      step value ``step``.\n",
            " |\n",
            " |      .. versionadded:: 1.4.0\n",
            " |\n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      start : int\n",
            " |          the start value\n",
            " |      end : int, optional\n",
            " |          the end value (exclusive)\n",
            " |      step : int, optional\n",
            " |          the incremental step (default: 1)\n",
            " |      numPartitions : int, optional\n",
            " |          the number of partitions of the DataFrame\n",
            " |\n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`DataFrame`\n",
            " |\n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> sqlContext.range(1, 7, 2).collect()\n",
            " |      [Row(id=1), Row(id=3), Row(id=5)]\n",
            " |\n",
            " |      If only one argument is specified, it will be used as the end value.\n",
            " |\n",
            " |      >>> sqlContext.range(3).collect()\n",
            " |      [Row(id=0), Row(id=1), Row(id=2)]\n",
            " |\n",
            " |  registerDataFrameAsTable(self, df: pyspark.sql.dataframe.DataFrame, tableName: str) -> None\n",
            " |      Registers the given :class:`DataFrame` as a temporary table in the catalog.\n",
            " |\n",
            " |      Temporary tables exist only during the lifetime of this instance of :class:`SQLContext`.\n",
            " |\n",
            " |      .. versionadded:: 1.3.0\n",
            " |\n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> sqlContext.registerDataFrameAsTable(df, \"table1\")\n",
            " |\n",
            " |  registerFunction(self, name: str, f: Callable[..., Any], returnType: Optional[pyspark.sql.types.DataType] = None) -> 'UserDefinedFunctionLike'\n",
            " |      An alias for :func:`spark.udf.register`.\n",
            " |      See :meth:`pyspark.sql.UDFRegistration.register`.\n",
            " |\n",
            " |      .. versionadded:: 1.2.0\n",
            " |\n",
            " |      .. deprecated:: 2.3.0\n",
            " |          Use :func:`spark.udf.register` instead.\n",
            " |\n",
            " |  registerJavaFunction(self, name: str, javaClassName: str, returnType: Optional[pyspark.sql.types.DataType] = None) -> None\n",
            " |      An alias for :func:`spark.udf.registerJavaFunction`.\n",
            " |      See :meth:`pyspark.sql.UDFRegistration.registerJavaFunction`.\n",
            " |\n",
            " |      .. versionadded:: 2.1.0\n",
            " |\n",
            " |      .. deprecated:: 2.3.0\n",
            " |          Use :func:`spark.udf.registerJavaFunction` instead.\n",
            " |\n",
            " |  setConf(self, key: str, value: Union[bool, int, str]) -> None\n",
            " |      Sets the given Spark SQL configuration property.\n",
            " |\n",
            " |      .. versionadded:: 1.3.0\n",
            " |\n",
            " |  sql(self, sqlQuery: str) -> pyspark.sql.dataframe.DataFrame\n",
            " |      Returns a :class:`DataFrame` representing the result of the given query.\n",
            " |\n",
            " |      .. versionadded:: 1.0.0\n",
            " |\n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`DataFrame`\n",
            " |\n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> sqlContext.registerDataFrameAsTable(df, \"table1\")\n",
            " |      >>> df2 = sqlContext.sql(\"SELECT field1 AS f1, field2 as f2 from table1\")\n",
            " |      >>> df2.collect()\n",
            " |      [Row(f1=1, f2='row1'), Row(f1=2, f2='row2'), Row(f1=3, f2='row3')]\n",
            " |\n",
            " |  table(self, tableName: str) -> pyspark.sql.dataframe.DataFrame\n",
            " |      Returns the specified table or view as a :class:`DataFrame`.\n",
            " |\n",
            " |      .. versionadded:: 1.0.0\n",
            " |\n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`DataFrame`\n",
            " |\n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> sqlContext.registerDataFrameAsTable(df, \"table1\")\n",
            " |      >>> df2 = sqlContext.table(\"table1\")\n",
            " |      >>> sorted(df.collect()) == sorted(df2.collect())\n",
            " |      True\n",
            " |\n",
            " |  tableNames(self, dbName: Optional[str] = None) -> List[str]\n",
            " |      Returns a list of names of tables in the database ``dbName``.\n",
            " |\n",
            " |      .. versionadded:: 1.3.0\n",
            " |\n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      dbName: str\n",
            " |          name of the database to use. Default to the current database.\n",
            " |\n",
            " |      Returns\n",
            " |      -------\n",
            " |      list\n",
            " |          list of table names, in string\n",
            " |\n",
            " |      >>> sqlContext.registerDataFrameAsTable(df, \"table1\")\n",
            " |      >>> \"table1\" in sqlContext.tableNames()\n",
            " |      True\n",
            " |      >>> \"table1\" in sqlContext.tableNames(\"default\")\n",
            " |      True\n",
            " |\n",
            " |  tables(self, dbName: Optional[str] = None) -> pyspark.sql.dataframe.DataFrame\n",
            " |      Returns a :class:`DataFrame` containing names of tables in the given database.\n",
            " |\n",
            " |      If ``dbName`` is not specified, the current database will be used.\n",
            " |\n",
            " |      The returned DataFrame has two columns: ``tableName`` and ``isTemporary``\n",
            " |      (a column with :class:`BooleanType` indicating if a table is a temporary one or not).\n",
            " |\n",
            " |      .. versionadded:: 1.3.0\n",
            " |\n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      dbName: str, optional\n",
            " |          name of the database to use.\n",
            " |\n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`DataFrame`\n",
            " |\n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> sqlContext.registerDataFrameAsTable(df, \"table1\")\n",
            " |      >>> df2 = sqlContext.tables()\n",
            " |      >>> df2.filter(\"tableName = 'table1'\").first()\n",
            " |      Row(namespace='', tableName='table1', isTemporary=True)\n",
            " |\n",
            " |  uncacheTable(self, tableName: str) -> None\n",
            " |      Removes the specified table from the in-memory cache.\n",
            " |\n",
            " |      .. versionadded:: 1.0\n",
            " |\n",
            " |  ----------------------------------------------------------------------\n",
            " |  Class methods defined here:\n",
            " |\n",
            " |  getOrCreate(sc: pyspark.context.SparkContext) -> 'SQLContext'\n",
            " |      Get the existing SQLContext or create a new one with given SparkContext.\n",
            " |\n",
            " |      .. versionadded:: 1.6.0\n",
            " |\n",
            " |      .. deprecated:: 3.0.0\n",
            " |          Use :func:`SparkSession.builder.getOrCreate()` instead.\n",
            " |\n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      sc : :class:`SparkContext`\n",
            " |\n",
            " |  ----------------------------------------------------------------------\n",
            " |  Readonly properties defined here:\n",
            " |\n",
            " |  read\n",
            " |      Returns a :class:`DataFrameReader` that can be used to read data\n",
            " |      in as a :class:`DataFrame`.\n",
            " |\n",
            " |      .. versionadded:: 1.4.0\n",
            " |\n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`DataFrameReader`\n",
            " |\n",
            " |  readStream\n",
            " |      Returns a :class:`DataStreamReader` that can be used to read data streams\n",
            " |      as a streaming :class:`DataFrame`.\n",
            " |\n",
            " |      .. versionadded:: 2.0.0\n",
            " |\n",
            " |      Notes\n",
            " |      -----\n",
            " |      This API is evolving.\n",
            " |\n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`DataStreamReader`\n",
            " |\n",
            " |      >>> text_sdf = sqlContext.readStream.text(tempfile.mkdtemp())\n",
            " |      >>> text_sdf.isStreaming\n",
            " |      True\n",
            " |\n",
            " |  streams\n",
            " |      Returns a :class:`StreamingQueryManager` that allows managing all the\n",
            " |      :class:`StreamingQuery` StreamingQueries active on `this` context.\n",
            " |\n",
            " |      .. versionadded:: 2.0.0\n",
            " |\n",
            " |      Notes\n",
            " |      -----\n",
            " |      This API is evolving.\n",
            " |\n",
            " |  udf\n",
            " |      Returns a :class:`UDFRegistration` for UDF registration.\n",
            " |\n",
            " |      .. versionadded:: 1.3.1\n",
            " |\n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`UDFRegistration`\n",
            " |\n",
            " |  udtf\n",
            " |      Returns a :class:`UDTFRegistration` for UDTF registration.\n",
            " |\n",
            " |      .. versionadded:: 3.5.0\n",
            " |\n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`UDTFRegistration`\n",
            " |\n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors defined here:\n",
            " |\n",
            " |  __dict__\n",
            " |      dictionary for instance variables\n",
            " |\n",
            " |  __weakref__\n",
            " |      list of weak references to the object\n",
            " |\n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data and other attributes defined here:\n",
            " |\n",
            " |  __annotations__ = {'_instantiatedContext': typing.ClassVar[typing.Opti...\n",
            "\n"
          ]
        }
      ],
      "source": [
        "help(sqlContext)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ReJOY-xkh3Mz"
      },
      "source": [
        "Once we have our RDD of `Row` we can infer and register the schema.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "lyVjzbF4h3M0"
      },
      "outputs": [],
      "source": [
        "#Creates a DataFrame from an RDD, a list or a pandas.DataFrame.\n",
        "interactions_df = sqlContext.createDataFrame(row_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "I-CjscEDh3M1"
      },
      "outputs": [],
      "source": [
        "#Registers this RDD as a temporary table using the given name.\n",
        "interactions_df.registerTempTable(\"interactions\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XxoWQb44h3M2"
      },
      "source": [
        "Now we can run SQL queries over our data frame that has been registered as a table.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "Rq2JNE3th3M3",
        "outputId": "b4d2870c-acf8-4b24-f282-462a7ef59761",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+---------+\n",
            "|duration|dst_bytes|\n",
            "+--------+---------+\n",
            "|    5057|        0|\n",
            "|    5059|        0|\n",
            "|    5051|        0|\n",
            "|    5056|        0|\n",
            "|    5051|        0|\n",
            "|    5039|        0|\n",
            "|    5062|        0|\n",
            "|    5041|        0|\n",
            "|    5056|        0|\n",
            "|    5064|        0|\n",
            "|    5043|        0|\n",
            "|    5061|        0|\n",
            "|    5049|        0|\n",
            "|    5061|        0|\n",
            "|    5048|        0|\n",
            "|    5047|        0|\n",
            "|    5044|        0|\n",
            "|    5063|        0|\n",
            "|    5068|        0|\n",
            "|    5062|        0|\n",
            "+--------+---------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Select tcp network interactions with more than 1 second duration and no transfer from destination\n",
        "# The sqlContext is uesd to returns a DataFrame representing the result of the given query.\n",
        "tcp_interactions = sqlContext.sql(\"\"\"\n",
        "    SELECT duration, dst_bytes FROM interactions WHERE protocol_type = 'tcp' AND duration > 1000 AND dst_bytes = 0\n",
        "\"\"\")\n",
        "tcp_interactions.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35qDetV_h3M4"
      },
      "source": [
        "The results of SQL queries are RDDs and support all the normal RDD operations.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "YCwynY9Nh3M4",
        "outputId": "99c4d864-0972-49bc-978a-89440e935b00",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Duration: 5057, Dest. bytes: 0\n",
            "Duration: 5059, Dest. bytes: 0\n",
            "Duration: 5051, Dest. bytes: 0\n",
            "Duration: 5056, Dest. bytes: 0\n",
            "Duration: 5051, Dest. bytes: 0\n",
            "Duration: 5039, Dest. bytes: 0\n",
            "Duration: 5062, Dest. bytes: 0\n",
            "Duration: 5041, Dest. bytes: 0\n",
            "Duration: 5056, Dest. bytes: 0\n",
            "Duration: 5064, Dest. bytes: 0\n",
            "Duration: 5043, Dest. bytes: 0\n",
            "Duration: 5061, Dest. bytes: 0\n",
            "Duration: 5049, Dest. bytes: 0\n",
            "Duration: 5061, Dest. bytes: 0\n",
            "Duration: 5048, Dest. bytes: 0\n",
            "Duration: 5047, Dest. bytes: 0\n",
            "Duration: 5044, Dest. bytes: 0\n",
            "Duration: 5063, Dest. bytes: 0\n",
            "Duration: 5068, Dest. bytes: 0\n",
            "Duration: 5062, Dest. bytes: 0\n",
            "Duration: 5046, Dest. bytes: 0\n",
            "Duration: 5052, Dest. bytes: 0\n",
            "Duration: 5044, Dest. bytes: 0\n",
            "Duration: 5054, Dest. bytes: 0\n",
            "Duration: 5039, Dest. bytes: 0\n",
            "Duration: 5058, Dest. bytes: 0\n",
            "Duration: 5051, Dest. bytes: 0\n",
            "Duration: 5032, Dest. bytes: 0\n",
            "Duration: 5063, Dest. bytes: 0\n",
            "Duration: 5040, Dest. bytes: 0\n",
            "Duration: 5051, Dest. bytes: 0\n",
            "Duration: 5066, Dest. bytes: 0\n",
            "Duration: 5044, Dest. bytes: 0\n",
            "Duration: 5051, Dest. bytes: 0\n",
            "Duration: 5036, Dest. bytes: 0\n",
            "Duration: 5055, Dest. bytes: 0\n",
            "Duration: 2426, Dest. bytes: 0\n",
            "Duration: 5047, Dest. bytes: 0\n",
            "Duration: 5057, Dest. bytes: 0\n",
            "Duration: 5037, Dest. bytes: 0\n",
            "Duration: 5057, Dest. bytes: 0\n",
            "Duration: 5062, Dest. bytes: 0\n",
            "Duration: 5051, Dest. bytes: 0\n",
            "Duration: 5051, Dest. bytes: 0\n",
            "Duration: 5053, Dest. bytes: 0\n",
            "Duration: 5064, Dest. bytes: 0\n",
            "Duration: 5044, Dest. bytes: 0\n",
            "Duration: 5051, Dest. bytes: 0\n",
            "Duration: 5033, Dest. bytes: 0\n",
            "Duration: 5066, Dest. bytes: 0\n",
            "Duration: 5063, Dest. bytes: 0\n",
            "Duration: 5056, Dest. bytes: 0\n",
            "Duration: 5042, Dest. bytes: 0\n",
            "Duration: 5063, Dest. bytes: 0\n",
            "Duration: 5060, Dest. bytes: 0\n",
            "Duration: 5056, Dest. bytes: 0\n",
            "Duration: 5049, Dest. bytes: 0\n",
            "Duration: 5043, Dest. bytes: 0\n",
            "Duration: 5039, Dest. bytes: 0\n",
            "Duration: 5041, Dest. bytes: 0\n",
            "Duration: 42448, Dest. bytes: 0\n",
            "Duration: 42088, Dest. bytes: 0\n",
            "Duration: 41065, Dest. bytes: 0\n",
            "Duration: 40929, Dest. bytes: 0\n",
            "Duration: 40806, Dest. bytes: 0\n",
            "Duration: 40682, Dest. bytes: 0\n",
            "Duration: 40571, Dest. bytes: 0\n",
            "Duration: 40448, Dest. bytes: 0\n",
            "Duration: 40339, Dest. bytes: 0\n",
            "Duration: 40232, Dest. bytes: 0\n",
            "Duration: 40121, Dest. bytes: 0\n",
            "Duration: 36783, Dest. bytes: 0\n",
            "Duration: 36674, Dest. bytes: 0\n",
            "Duration: 36570, Dest. bytes: 0\n",
            "Duration: 36467, Dest. bytes: 0\n",
            "Duration: 36323, Dest. bytes: 0\n",
            "Duration: 36204, Dest. bytes: 0\n",
            "Duration: 32038, Dest. bytes: 0\n",
            "Duration: 31925, Dest. bytes: 0\n",
            "Duration: 31809, Dest. bytes: 0\n",
            "Duration: 31709, Dest. bytes: 0\n",
            "Duration: 31601, Dest. bytes: 0\n",
            "Duration: 31501, Dest. bytes: 0\n",
            "Duration: 31401, Dest. bytes: 0\n",
            "Duration: 31301, Dest. bytes: 0\n",
            "Duration: 31194, Dest. bytes: 0\n",
            "Duration: 31061, Dest. bytes: 0\n",
            "Duration: 30935, Dest. bytes: 0\n",
            "Duration: 30835, Dest. bytes: 0\n",
            "Duration: 30735, Dest. bytes: 0\n",
            "Duration: 30619, Dest. bytes: 0\n",
            "Duration: 30518, Dest. bytes: 0\n",
            "Duration: 30418, Dest. bytes: 0\n",
            "Duration: 30317, Dest. bytes: 0\n",
            "Duration: 30217, Dest. bytes: 0\n",
            "Duration: 30077, Dest. bytes: 0\n",
            "Duration: 25420, Dest. bytes: 0\n",
            "Duration: 22921, Dest. bytes: 0\n",
            "Duration: 22821, Dest. bytes: 0\n",
            "Duration: 22721, Dest. bytes: 0\n",
            "Duration: 22616, Dest. bytes: 0\n",
            "Duration: 22516, Dest. bytes: 0\n",
            "Duration: 22416, Dest. bytes: 0\n",
            "Duration: 22316, Dest. bytes: 0\n",
            "Duration: 22216, Dest. bytes: 0\n",
            "Duration: 21987, Dest. bytes: 0\n",
            "Duration: 21887, Dest. bytes: 0\n",
            "Duration: 21767, Dest. bytes: 0\n",
            "Duration: 21661, Dest. bytes: 0\n",
            "Duration: 21561, Dest. bytes: 0\n",
            "Duration: 21455, Dest. bytes: 0\n",
            "Duration: 21334, Dest. bytes: 0\n",
            "Duration: 21223, Dest. bytes: 0\n",
            "Duration: 21123, Dest. bytes: 0\n",
            "Duration: 20983, Dest. bytes: 0\n",
            "Duration: 14682, Dest. bytes: 0\n",
            "Duration: 14420, Dest. bytes: 0\n",
            "Duration: 14319, Dest. bytes: 0\n",
            "Duration: 14198, Dest. bytes: 0\n",
            "Duration: 14098, Dest. bytes: 0\n",
            "Duration: 13998, Dest. bytes: 0\n",
            "Duration: 13898, Dest. bytes: 0\n",
            "Duration: 13796, Dest. bytes: 0\n",
            "Duration: 13678, Dest. bytes: 0\n",
            "Duration: 13578, Dest. bytes: 0\n",
            "Duration: 13448, Dest. bytes: 0\n",
            "Duration: 13348, Dest. bytes: 0\n",
            "Duration: 13241, Dest. bytes: 0\n",
            "Duration: 13141, Dest. bytes: 0\n",
            "Duration: 13033, Dest. bytes: 0\n",
            "Duration: 12933, Dest. bytes: 0\n",
            "Duration: 12833, Dest. bytes: 0\n",
            "Duration: 12733, Dest. bytes: 0\n",
            "Duration: 12001, Dest. bytes: 0\n",
            "Duration: 5678, Dest. bytes: 0\n",
            "Duration: 5010, Dest. bytes: 0\n",
            "Duration: 1298, Dest. bytes: 0\n",
            "Duration: 1031, Dest. bytes: 0\n",
            "Duration: 36438, Dest. bytes: 0\n"
          ]
        }
      ],
      "source": [
        "# Output duration together with dst_bytes\n",
        "# Return a new RDD by applying a function to each element of this RDD.\n",
        "tcp_interactions_out = tcp_interactions.rdd.map(lambda p: \"Duration: {}, Dest. bytes: {}\".format(p.duration, p.dst_bytes))\n",
        "for ti_out in tcp_interactions_out.collect():\n",
        "    print(ti_out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1j30aDVh3M6"
      },
      "source": [
        "We can easily have a look at our data frame schema using `printSchema`.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "5G2gKHWsh3M6",
        "outputId": "287ce6de-96b4-47d9-8101-642e2eef08e2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- duration: long (nullable = true)\n",
            " |-- protocol_type: string (nullable = true)\n",
            " |-- service: string (nullable = true)\n",
            " |-- flag: string (nullable = true)\n",
            " |-- src_bytes: long (nullable = true)\n",
            " |-- dst_bytes: long (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "interactions_df.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PwyuGRLPh3M8"
      },
      "source": [
        "### 8.Queries as `DataFrame` operations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JRtyQgqdh3M-"
      },
      "source": [
        "Spark `DataFrame` provides a domain-specific language for structured data manipulation. This language includes methods we can concatenate in order to do selection, filtering, grouping, etc. For example, let's say we want to count how many interactions are there for each protocol type. We can proceed as follows.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "PirwiZv4h3M_",
        "outputId": "d8b9c543-10dd-4389-e7cd-94cdcae16f6f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+------+\n",
            "|protocol_type| count|\n",
            "+-------------+------+\n",
            "|          tcp|190065|\n",
            "|          udp| 20354|\n",
            "|         icmp|283602|\n",
            "+-------------+------+\n",
            "\n",
            "Query performed in 6.575 seconds\n"
          ]
        }
      ],
      "source": [
        "from time import time\n",
        "\n",
        "t0 = time()\n",
        "interactions_df.select(\"protocol_type\", \"duration\", \"dst_bytes\").groupBy(\"protocol_type\").count().show()\n",
        "tt = time() - t0\n",
        "\n",
        "print(\"Query performed in {} seconds\".format(round(tt,3)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Put6CVcBh3NA"
      },
      "source": [
        "Now imagine that we want to count how many interactions last more than 1 second, with no data transfer from destination, grouped by protocol type. We can just add to filter calls to the previous.   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "Mj8HXB-6h3NB",
        "outputId": "318cd134-d244-42f8-c36d-0164308b25f7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+-----+\n",
            "|protocol_type|count|\n",
            "+-------------+-----+\n",
            "|          tcp|  139|\n",
            "+-------------+-----+\n",
            "\n",
            "Query performed in 5.57 seconds\n"
          ]
        }
      ],
      "source": [
        "t0 = time()\n",
        "interactions_df.select(\"protocol_type\", \"duration\", \"dst_bytes\").filter(interactions_df.duration>1000).filter(interactions_df.dst_bytes==0).groupBy(\"protocol_type\").count().show()\n",
        "tt = time() - t0\n",
        "\n",
        "print(\"Query performed in {} seconds\".format(round(tt,3)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YFdljE5fh3NC"
      },
      "source": [
        "We can use this to perform some [exploratory data analysis](http://en.wikipedia.org/wiki/Exploratory_data_analysis). Let's count how many attack and normal interactions we have. First we need to add the label column to our data.    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "bCQx7bX3h3ND"
      },
      "outputs": [],
      "source": [
        "def get_label_type(label):\n",
        "    if label!=\"normal.\":\n",
        "        return \"attack\"\n",
        "    else:\n",
        "        return \"normal\"\n",
        "\n",
        "row_labeled_data = csv_data.map(lambda p: Row(\n",
        "    duration=int(p[0]),\n",
        "    protocol_type=p[1],\n",
        "    service=p[2],\n",
        "    flag=p[3],\n",
        "    src_bytes=int(p[4]),\n",
        "    dst_bytes=int(p[5]),\n",
        "    label=get_label_type(p[41])\n",
        "    )\n",
        ")\n",
        "interactions_labeled_df = sqlContext.createDataFrame(row_labeled_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VaIxW6znh3NF"
      },
      "source": [
        "This time we don't need to register the schema since we are going to use the OO query interface.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_z8_n1Kh3NF"
      },
      "source": [
        "Let's check the previous actually works by counting attack and normal data in our data frame.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "Q31tE2sgh3NF",
        "outputId": "ba965d33-7a1b-4e80-f34a-0cd17308f286",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+------+\n",
            "| label| count|\n",
            "+------+------+\n",
            "|normal| 97278|\n",
            "|attack|396743|\n",
            "+------+------+\n",
            "\n",
            "Query performed in 5.003 seconds\n"
          ]
        }
      ],
      "source": [
        "t0 = time()\n",
        "interactions_labeled_df.select(\"label\").groupBy(\"label\").count().show()\n",
        "tt = time() - t0\n",
        "\n",
        "print(\"Query performed in {} seconds\".format(round(tt,3)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-POnaJAUh3NH"
      },
      "source": [
        "Now we want to count them by label and protocol type, in order to see how important the protocol type is to detect when an interaction is or not an attack.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "LfQgEhhvh3NI",
        "outputId": "5fb949ac-d156-4ddf-dd11-81acc14e7d63",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-------------+------+\n",
            "| label|protocol_type| count|\n",
            "+------+-------------+------+\n",
            "|normal|          udp| 19177|\n",
            "|normal|         icmp|  1288|\n",
            "|normal|          tcp| 76813|\n",
            "|attack|         icmp|282314|\n",
            "|attack|          tcp|113252|\n",
            "|attack|          udp|  1177|\n",
            "+------+-------------+------+\n",
            "\n",
            "Query performed in 5.633 seconds\n"
          ]
        }
      ],
      "source": [
        "t0 = time()\n",
        "interactions_labeled_df.select(\"label\", \"protocol_type\").groupBy(\"label\", \"protocol_type\").count().show()\n",
        "tt = time() - t0\n",
        "\n",
        "print(\"Query performed in {} seconds\".format(round(tt,3)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XmryXB3kh3NK"
      },
      "source": [
        "At first sight it seems that *udp* interactions are in lower proportion between network attacks versus other protocol types.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8Ub4yuEh3NL"
      },
      "source": [
        "And we can do much more sofisticated groupings. For example, add to the previous a \"split\" based on data transfer from target.   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "8u0t0an8h3NL",
        "outputId": "7a23bee3-a336-4bfc-9586-c8811a0bb111",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-------------+---------------+------+\n",
            "| label|protocol_type|(dst_bytes = 0)| count|\n",
            "+------+-------------+---------------+------+\n",
            "|normal|          udp|          false| 15583|\n",
            "|attack|          udp|          false|    11|\n",
            "|attack|          tcp|           true|110583|\n",
            "|normal|          tcp|          false| 67500|\n",
            "|attack|         icmp|           true|282314|\n",
            "|attack|          tcp|          false|  2669|\n",
            "|normal|          tcp|           true|  9313|\n",
            "|normal|          udp|           true|  3594|\n",
            "|normal|         icmp|           true|  1288|\n",
            "|attack|          udp|           true|  1166|\n",
            "+------+-------------+---------------+------+\n",
            "\n",
            "Query performed in 5.725 seconds\n"
          ]
        }
      ],
      "source": [
        "t0 = time()\n",
        "interactions_labeled_df.select(\"label\", \"protocol_type\", \"dst_bytes\").groupBy(\"label\", \"protocol_type\", interactions_labeled_df.dst_bytes==0).count().show()\n",
        "tt = time() - t0\n",
        "\n",
        "print(\"Query performed in {} seconds\".format(round(tt,3)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIq3qh9zh3NM"
      },
      "source": [
        "We see how relevant is this new split to determine if a network interaction is an attack.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCbeFsx5h3NM"
      },
      "source": [
        "We will stop here, but we can see how powerfull this type of queries are in order to explore our data. Actually we can replicate all the splits we saw in previous notebooks, when introducing classification trees, just by selecting, groping, and filtering our dataframe. For a more detailed (but less real-world) list of Spark's `DataFrame` operations and data sources, have a look at the oficial documentation [here](https://spark.apache.org/docs/latest/sql-programming-guide.html#dataframe-operations).    "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}